{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "96fa86e0",
      "metadata": {},
      "source": [
        "### The CIFAR-10 dataset\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f159036",
      "metadata": {},
      "outputs": [],
      "source": [
        "# supress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\".*sklearn.*\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0efbe769",
      "metadata": {},
      "source": [
        "## EDA\n",
        "can be further improved later on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1cfdd6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# from sklearn.manifold import TSNE # Visual cluster structure—see how classes separate in 2D\n",
        "# import numpy as np\n",
        "\n",
        "# # 1. Load CIFAR-10\n",
        "# transform = transforms.Compose([transforms.ToTensor()])\n",
        "# trainset = torchvision.datasets.CIFAR10(\n",
        "#     root='./data', train=True, download=True, transform=transform)\n",
        "# classes = trainset.classes\n",
        "# targets = torch.tensor(trainset.targets)\n",
        "\n",
        "# # 2. Class counts => Ensures your data is balanced (it is, *5,000* images per class).\n",
        "# class_counts = [(classes[i], int((targets == i).sum().item()))\n",
        "#                 for i in range(len(classes))]\n",
        "# df_counts = pd.DataFrame(class_counts, columns=['Class', 'Count'])\n",
        "# print(df_counts)\n",
        "\n",
        "# # 3. Compute per-channel mean & std => Useful for normalization before training.\n",
        "# loader = torch.utils.data.DataLoader(trainset, batch_size=5000, shuffle=False, num_workers=2)\n",
        "# mean = 0.\n",
        "# std = 0.\n",
        "# n_samples = 0\n",
        "# for data, _ in loader:\n",
        "#     bs = data.size(0)\n",
        "#     data = data.view(bs, data.size(1), -1)\n",
        "#     mean += data.mean(2).sum(0)\n",
        "#     std  += data.std(2).sum(0)\n",
        "#     n_samples += bs\n",
        "# mean /= n_samples\n",
        "# std  /= n_samples\n",
        "# print(f\"Mean per channel: {mean.tolist()}\")\n",
        "# print(f\"Std  per channel: {std.tolist()}\")\n",
        "\n",
        "# # 4. Plot Class Distribution\n",
        "# plt.figure(figsize=(8,4))\n",
        "# plt.bar(df_counts['Class'], df_counts['Count'])\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.title(\"CIFAR-10 Class Distribution\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # 5. Pixel-value Histograms per Channel\n",
        "# # Check for any channel biases or artifacts.\n",
        "# # Stack a subset of images to save time\n",
        "# subset = torch.stack([trainset[i][0] for i in range(10000)])  # 10k images\n",
        "# for ch, col in enumerate(['Red', 'Green', 'Blue']):\n",
        "#     plt.figure()\n",
        "#     plt.hist(subset[:, ch, :, :].numpy().ravel(), bins=50)\n",
        "#     plt.title(f\"{col} Channel Histogram (10k samples)\")\n",
        "#     plt.xlabel(\"Pixel value\")\n",
        "#     plt.ylabel(\"Frequency\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # 6. t-SNE Embedding (flattened, on 2000 random images) => t-distributed Stochastic Neighbor Embedding\n",
        "# np.random.seed(42)\n",
        "# idxs = np.random.choice(len(trainset), 2000, replace=False)\n",
        "# data_flat = np.stack([trainset[i][0].numpy().ravel() for i in idxs])\n",
        "# labels   = [trainset[i][1] for i in idxs]\n",
        "# tsne = TSNE(n_components=2, perplexity=30, random_state=0)\n",
        "# emb = tsne.fit_transform(data_flat)\n",
        "\n",
        "# plt.figure(figsize=(6,6))\n",
        "# scatter = plt.scatter(emb[:,0], emb[:,1], c=labels, alpha=0.6, cmap='tab10')\n",
        "# plt.legend(handles=scatter.legend_elements()[0], labels=classes, bbox_to_anchor=(1.05,1))\n",
        "# plt.title(\"t-SNE of CIFAR-10 (2k samples)\")\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# # 7. Sample Grid => A quick sanity check of raw images. \n",
        "# fig, axes = plt.subplots(4,8, figsize=(12,6))\n",
        "# for ax in axes.flatten():\n",
        "#     i = torch.randint(len(trainset), (1,)).item()\n",
        "#     img, lbl = trainset[i]\n",
        "#     img = img.permute(1,2,0).numpy()\n",
        "#     ax.imshow(img)\n",
        "#     ax.set_title(classes[lbl], fontsize=8)\n",
        "#     ax.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410b0c18",
      "metadata": {
        "id": "410b0c18"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8aa7d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe8aa7d6",
        "outputId": "f4253cae-314d-4de8-8211-eec67e8e7a9f"
      },
      "outputs": [],
      "source": [
        "# Create directories for saving results\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"visualizations\", exist_ok=True)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce935e1",
      "metadata": {
        "id": "7ce935e1"
      },
      "source": [
        "## ADJUSTABLE PARAMETERS\n",
        "Try changing the following parameters to see how they affect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3baf6528",
      "metadata": {
        "id": "3baf6528"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "# Model selection (set to True to train)\n",
        "TRAIN_ALEXNET = True\n",
        "TRAIN_VGG16 = True\n",
        "TRAIN_VGG16_BN = True\n",
        "TRAIN_VGG8 = True\n",
        "\n",
        "# Architecture adjustments\n",
        "ALEXNET_FC_SIZE = 4096  # Size of AlexNet's first fully connected layer according to the paper\n",
        "VGG_FC_SIZE = 4096      # Size of VGG's first fully connected layer according to the paper\n",
        "USE_DROPOUT = True      # Whether to use dropout in fully connected layers\n",
        "DROPOUT_RATE = 0.5      # Dropout probability\n",
        "\n",
        "# Data augmentation settings\n",
        "USE_DATA_AUGMENTATION = True\n",
        "HORIZONTAL_FLIP = True\n",
        "RANDOM_CROP = True\n",
        "NORMALIZE_DATA = True\n",
        "\n",
        "# Visualization settings\n",
        "SAVE_FILTERS = True\n",
        "SAVE_FEATURE_MAPS = True\n",
        "SAVE_TRAINING_CURVES = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bee06ae",
      "metadata": {},
      "source": [
        "| Parameter                                                       | What it does                                                                                   | Typical effects of changing it                                                                                                                                            |\n",
        "| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **BATCH\\_SIZE**                                                 | Number of samples processed before the model’s weights are updated.                            | • Larger batch → smoother gradient estimates, faster GPU utilization, but more memory.<br>• Smaller batch → noisier updates, can generalize better, but slower per‑epoch. |\n",
        "| **NUM\\_EPOCHS**                                                 | How many full passes over the training set.                                                    | • More epochs → model can learn more, but risk overfitting.<br>• Fewer epochs → faster run but may underfit.                                                              |\n",
        "| **LEARNING\\_RATE**                                              | Step size for the optimizer when updating weights (controls how “big” each update is).         | • High LR → faster initial learning but risk of divergence.<br>• Low LR → stable convergence but slower training.                                                         |\n",
        "| **WEIGHT\\_DECAY**                                               | L2‐regularization coefficient (penalizes large weights).                                       | • Higher → stronger regularization (helps prevent overfitting).<br>• Lower → less regularization (can fit data better, but risk overfit).                                 |\n",
        "| **MOMENTUM**                                                    | In SGD, momentum keeps a fraction of the previous update to smooth and accelerate convergence. | • High (e.g. 0.9+) → faster convergence and can escape shallow minima.<br>• Low (e.g. 0.5) → more responsive to recent gradients but noisier.                             |\n",
        "| **TRAIN\\_ALEXNET, TRAIN\\_VGG16, TRAIN\\_VGG16\\_BN, TRAIN\\_VGG8** | Booleans to enable/disable training of each architecture.                                      | Turn each model on/off if you only want to run a subset of experiments.                                                                                                   |\n",
        "| **ALEXNET\\_FC\\_SIZE**                                           | Number of neurons in AlexNet’s first fully‑connected (FC) layer.                               | • Larger → more capacity, but more parameters and risk of overfitting.<br>• Smaller → faster training, fewer parameters.                                                  |\n",
        "| **VGG\\_FC\\_SIZE**                                               | Number of neurons in each of VGG’s FC layers.                                                  | Similar trade‑off as ALEXNET\\_FC\\_SIZE for VGG architectures.                                                                                                             |\n",
        "| **USE\\_DROPOUT**                                                | Whether to include dropout layers in the classifier head.                                      | • True → adds regularization by randomly zeroing connections each batch.<br>• False → no dropout (faster but less robust).                                                |\n",
        "| **DROPOUT\\_RATE**                                               | Probability that each neuron is “dropped” (set to zero) during training.                       | • Higher (e.g. 0.7) → stronger regularization.<br>• Lower (e.g. 0.3) → milder regularization.                                                                             |\n",
        "| **USE\\_DATA\\_AUGMENTATION**                                     | Master switch for whether to apply random crops & flips to training images.                    | • True → generally improves generalization by exposing model to varied views.<br>• False → training on only original images.                                              |\n",
        "| **HORIZONTAL\\_FLIP**                                            | If augmenting, whether to randomly flip images left–right.                                     | • Often safe for natural images (e.g. cars, animals).                                                                                                                     |\n",
        "| **RANDOM\\_CROP**                                                | If augmenting, whether to randomly crop (with padding) around the image.                       | • Introduces translation invariance.                                                                                                                                      |\n",
        "| **NORMALIZE\\_DATA**                                             | Whether to subtract the dataset mean and divide by the std per channel.                        | • Almost always recommended for stable / faster convergence.<br>• Can be turned off when feeding raw pixels.                                                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "307e27ee",
      "metadata": {},
      "source": [
        "```\n",
        "Parameter                | Example values to try\n",
        "BATCH_SIZE               | 64, 128, 256         \n",
        "NUM_EPOCHS               | 40, 60, 80           \n",
        "LEARNING_RATE            | 0.1, 0.01, 0.001     \n",
        "WEIGHT_DECAY             | 1e-3, 5e-4, 1e-4     \n",
        "MOMENTUM                 | 0.9, 0.95, 0.99      \n",
        "ALEXNET_FC_SIZE          | 4096, 2048, 1024     \n",
        "VGG_FC_SIZE              | 4096, 2048, 1024     \n",
        "DROPOUT_RATE             | 0.3, 0.5, 0.7        \n",
        "USE_DROPOUT              | True vs. False       \n",
        "USE_DATA_AUGMENTATION    | True vs. False       \n",
        "HORIZONTAL_FLIP          | True vs. False       \n",
        "RANDOM_CROP              | True vs. False       \n",
        "NORMALIZE_DATA           | True vs. False       \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zq-vmqbX386w",
      "metadata": {
        "id": "zq-vmqbX386w"
      },
      "source": [
        "## DATA LOADING AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qmY9yCDX35ep",
      "metadata": {
        "id": "qmY9yCDX35ep"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "if USE_DATA_AUGMENTATION:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4) if RANDOM_CROP else transforms.Lambda(lambda x: x),\n",
        "        transforms.RandomHorizontalFlip() if HORIZONTAL_FLIP else transforms.Lambda(lambda x: x),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)) if NORMALIZE_DATA else transforms.Lambda(lambda x: x),\n",
        "    ])\n",
        "else:\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)) if NORMALIZE_DATA else transforms.Lambda(lambda x: x),\n",
        "    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)) if NORMALIZE_DATA else transforms.Lambda(lambda x: x),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR-10\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1834e1e",
      "metadata": {
        "id": "a1834e1e"
      },
      "source": [
        "## MODEL DEFINITIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ced251a",
      "metadata": {
        "id": "5ced251a"
      },
      "outputs": [],
      "source": [
        "# AlexNet architecture adapted for CIFAR-10\n",
        "class AlexNetCIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNetCIFAR, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv2\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv3\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv4\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv5\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        if USE_DROPOUT:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(256 * 4 * 4, ALEXNET_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(ALEXNET_FC_SIZE, 1024),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(1024, num_classes),\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(256 * 4 * 4, ALEXNET_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(ALEXNET_FC_SIZE, 1024),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(1024, num_classes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG-16 architecture for CIFAR-10\n",
        "class VGG16CIFAR(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16CIFAR, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        if USE_DROPOUT:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(VGG_FC_SIZE, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(VGG_FC_SIZE, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG-16 with Batch Normalization\n",
        "class VGG16BN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG16BN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        if USE_DROPOUT:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(VGG_FC_SIZE, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(VGG_FC_SIZE, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# VGG-8 (Reduced depth)\n",
        "class VGG8(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG8, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        if USE_DROPOUT:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 2 * 2, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512 * 2 * 2, VGG_FC_SIZE),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(VGG_FC_SIZE, num_classes),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825fe0c9",
      "metadata": {
        "id": "825fe0c9"
      },
      "source": [
        "## TRAINING AND EVALUATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b162ffa4",
      "metadata": {
        "id": "b162ffa4"
      },
      "outputs": [],
      "source": [
        "# Function to count parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n",
        "    model = model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    times_per_epoch = []\n",
        "\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({'loss': loss.item(), 'acc': f\"{100 * running_corrects.double() / total:.2f}%\"})\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = running_corrects.double() / total\n",
        "\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accs.append(epoch_acc.item())\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(testloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "            for inputs, labels in pbar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({'loss': loss.item(), 'acc': f\"{100 * running_corrects.double() / total:.2f}%\"})\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = running_corrects.double() / total\n",
        "\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_accs.append(epoch_acc.item())\n",
        "\n",
        "        # Save the best model\n",
        "        if epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        epoch_time = time.time() - start_time\n",
        "        times_per_epoch.append(epoch_time)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accs[-1]:.4f}')\n",
        "        print(f'Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accs[-1]:.4f}')\n",
        "        print(f'Time: {epoch_time:.2f}s')\n",
        "        print('-' * 50)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_accs, val_accs, times_per_epoch\n",
        "\n",
        "# Function to evaluate model on test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccbd14a",
      "metadata": {
        "id": "7ccbd14a"
      },
      "source": [
        "## VISUALIZATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ed2326d",
      "metadata": {
        "id": "9ed2326d"
      },
      "outputs": [],
      "source": [
        "# Function to visualize first layer filters\n",
        "def visualize_filters(model, title):\n",
        "    if not SAVE_FILTERS:\n",
        "        return\n",
        "\n",
        "    # Get the first convolutional layer\n",
        "    first_layer = None\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            first_layer = module\n",
        "            break\n",
        "\n",
        "    if first_layer is None:\n",
        "        print(\"No convolutional layer found\")\n",
        "        return\n",
        "\n",
        "    # Get the weights of the first layer\n",
        "    weights = first_layer.weight.data.cpu().numpy()\n",
        "\n",
        "    # Plotting\n",
        "    fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "    fig.suptitle(f'First Layer Filters - {title}', fontsize=16)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        if i < weights.shape[0]:  # Only plot if filter exists\n",
        "            # Normalize filter for better visualization\n",
        "            img = weights[i, 0, :, :]  # Get the first channel (R)\n",
        "            img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "            ax.imshow(img, cmap='viridis')\n",
        "            ax.set_title(f'Filter {i+1}')\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.savefig(f'visualizations/{title}_filters.png')\n",
        "    plt.close()\n",
        "\n",
        "# Function to visualize feature maps\n",
        "def visualize_feature_maps(model, title):\n",
        "    if not SAVE_FEATURE_MAPS:\n",
        "        return\n",
        "\n",
        "    # Register a hook to get feature maps\n",
        "    feature_maps = {}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        feature_maps['features'] = output.detach().cpu()\n",
        "\n",
        "    # Get the first convolutional layer\n",
        "    first_conv = None\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            first_conv = module\n",
        "            break\n",
        "\n",
        "    if first_conv is None:\n",
        "        print(\"No convolutional layer found\")\n",
        "        return\n",
        "\n",
        "    hook = first_conv.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Get a batch of images\n",
        "    dataiter = iter(testloader)\n",
        "    images, _ = next(dataiter)\n",
        "\n",
        "    # Forward pass\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = model(images[0:1].to(device))\n",
        "\n",
        "    # Remove the hook\n",
        "    hook.remove()\n",
        "\n",
        "    # Get feature maps\n",
        "    if 'features' in feature_maps:\n",
        "        feature_map = feature_maps['features'][0]\n",
        "\n",
        "        # Plot feature maps\n",
        "        fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
        "        fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "        fig.suptitle(f'Feature Maps - {title}', fontsize=16)\n",
        "\n",
        "        for i, ax in enumerate(axes.flat):\n",
        "            if i < feature_map.shape[0]:  # Only plot if feature map exists\n",
        "                img = feature_map[i].numpy()\n",
        "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "                ax.imshow(img, cmap='viridis')\n",
        "                ax.set_title(f'Map {i+1}')\n",
        "                ax.axis('off')\n",
        "            else:\n",
        "                ax.axis('off')\n",
        "\n",
        "        plt.savefig(f'visualizations/{title}_feature_maps.png')\n",
        "        plt.close()\n",
        "\n",
        "# Function to plot training history\n",
        "def plot_history(histories, model_names):\n",
        "    if not SAVE_TRAINING_CURVES:\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
        "        train_accs, val_accs, train_losses, val_losses, times = history\n",
        "        epochs = range(1, len(train_accs) + 1)\n",
        "\n",
        "        axes[0, 0].plot(epochs, train_accs, marker='o', linestyle='-', label=f'{name} Train')\n",
        "        axes[0, 0].set_title('Training Accuracy')\n",
        "        axes[0, 0].set_xlabel('Epochs')\n",
        "        axes[0, 0].set_ylabel('Accuracy')\n",
        "        axes[0, 0].grid(True)\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        axes[0, 1].plot(epochs, val_accs, marker='o', linestyle='-', label=f'{name} Val')\n",
        "        axes[0, 1].set_title('Validation Accuracy')\n",
        "        axes[0, 1].set_xlabel('Epochs')\n",
        "        axes[0, 1].set_ylabel('Accuracy')\n",
        "        axes[0, 1].grid(True)\n",
        "        axes[0, 1].legend()\n",
        "\n",
        "        axes[1, 0].plot(epochs, train_losses, marker='o', linestyle='-', label=f'{name} Train')\n",
        "        axes[1, 0].set_title('Training Loss')\n",
        "        axes[1, 0].set_xlabel('Epochs')\n",
        "        axes[1, 0].set_ylabel('Loss')\n",
        "        axes[1, 0].grid(True)\n",
        "        axes[1, 0].legend()\n",
        "\n",
        "        axes[1, 1].plot(epochs, val_losses, marker='o', linestyle='-', label=f'{name} Val')\n",
        "        axes[1, 1].set_title('Validation Loss')\n",
        "        axes[1, 1].set_xlabel('Epochs')\n",
        "        axes[1, 1].set_ylabel('Loss')\n",
        "        axes[1, 1].grid(True)\n",
        "        axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Plot time per epoch\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i, (name, history) in enumerate(zip(model_names, histories)):\n",
        "        _, _, _, _, times = history\n",
        "        epochs = range(1, len(times) + 1)\n",
        "        plt.plot(epochs, times, marker='o', linestyle='-', label=name)\n",
        "\n",
        "    plt.title('Time per Epoch')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Time (seconds)')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('visualizations/time_per_epoch.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0659340a",
      "metadata": {
        "id": "0659340a"
      },
      "source": [
        "## MAIN EXECUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd8c4b1",
      "metadata": {
        "id": "edd8c4b1"
      },
      "outputs": [],
      "source": [
        "def run_experiments():\n",
        "    histories = []\n",
        "    model_names = []\n",
        "    results = {}\n",
        "\n",
        "    # Initialize criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train and evaluate AlexNet\n",
        "    if TRAIN_ALEXNET:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Training AlexNet\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        alexnet_model = AlexNetCIFAR()\n",
        "        print(f\"AlexNet parameters: {count_parameters(alexnet_model):,}\")\n",
        "\n",
        "        alexnet_optimizer = optim.SGD(alexnet_model.parameters(),\n",
        "                                     lr=LEARNING_RATE,\n",
        "                                     momentum=MOMENTUM,\n",
        "                                     weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        alexnet_scheduler = optim.lr_scheduler.CosineAnnealingLR(alexnet_optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "        alexnet_model, alexnet_train_losses, alexnet_val_losses, alexnet_train_accs, alexnet_val_accs, alexnet_times = train_model(\n",
        "            alexnet_model, criterion, alexnet_optimizer, alexnet_scheduler, num_epochs=NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        torch.save(alexnet_model.state_dict(), 'results/alexnet_cifar10.pth')\n",
        "\n",
        "        # Evaluate final test accuracy\n",
        "        alexnet_acc = evaluate_model(alexnet_model, testloader)\n",
        "        print(f\"AlexNet Test Accuracy: {alexnet_acc:.2f}%\")\n",
        "\n",
        "        # Visualize filters and feature maps\n",
        "        visualize_filters(alexnet_model, \"AlexNet\")\n",
        "        visualize_feature_maps(alexnet_model, \"AlexNet\")\n",
        "\n",
        "        # Save results\n",
        "        histories.append((alexnet_train_accs, alexnet_val_accs, alexnet_train_losses, alexnet_val_losses, alexnet_times))\n",
        "        model_names.append(\"AlexNet\")\n",
        "\n",
        "        results['AlexNet'] = {\n",
        "            'parameters': count_parameters(alexnet_model),\n",
        "            'test_accuracy': alexnet_acc,\n",
        "            'train_accuracy': alexnet_train_accs[-1],\n",
        "            'val_accuracy': alexnet_val_accs[-1],\n",
        "            'time_per_epoch': sum(alexnet_times) / len(alexnet_times)\n",
        "        }\n",
        "\n",
        "    # Train and evaluate VGG-16\n",
        "    if TRAIN_VGG16:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Training VGG-16\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        vgg16_model = VGG16CIFAR()\n",
        "        print(f\"VGG-16 parameters: {count_parameters(vgg16_model):,}\")\n",
        "\n",
        "        vgg16_optimizer = optim.SGD(vgg16_model.parameters(),\n",
        "                                    lr=LEARNING_RATE,\n",
        "                                    momentum=MOMENTUM,\n",
        "                                    weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        vgg16_scheduler = optim.lr_scheduler.CosineAnnealingLR(vgg16_optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "        vgg16_model, vgg16_train_losses, vgg16_val_losses, vgg16_train_accs, vgg16_val_accs, vgg16_times = train_model(\n",
        "            vgg16_model, criterion, vgg16_optimizer, vgg16_scheduler, num_epochs=NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        torch.save(vgg16_model.state_dict(), 'results/vgg16_cifar10.pth')\n",
        "\n",
        "        # Evaluate final test accuracy\n",
        "        vgg16_acc = evaluate_model(vgg16_model, testloader)\n",
        "        print(f\"VGG-16 Test Accuracy: {vgg16_acc:.2f}%\")\n",
        "\n",
        "        # Visualize filters and feature maps\n",
        "        visualize_filters(vgg16_model, \"VGG-16\")\n",
        "        visualize_feature_maps(vgg16_model, \"VGG-16\")\n",
        "\n",
        "        # Save results\n",
        "        histories.append((vgg16_train_accs, vgg16_val_accs, vgg16_train_losses, vgg16_val_losses, vgg16_times))\n",
        "        model_names.append(\"VGG-16\")\n",
        "\n",
        "        results['VGG-16'] = {\n",
        "            'parameters': count_parameters(vgg16_model),\n",
        "            'test_accuracy': vgg16_acc,\n",
        "            'train_accuracy': vgg16_train_accs[-1],\n",
        "            'val_accuracy': vgg16_val_accs[-1],\n",
        "            'time_per_epoch': sum(vgg16_times) / len(vgg16_times)\n",
        "        }\n",
        "\n",
        "    # Train and evaluate VGG-16 with Batch Normalization\n",
        "    if TRAIN_VGG16_BN:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Training VGG-16 with Batch Normalization\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        vgg16bn_model = VGG16BN()\n",
        "        print(f\"VGG-16 BN parameters: {count_parameters(vgg16bn_model):,}\")\n",
        "\n",
        "        vgg16bn_optimizer = optim.SGD(vgg16bn_model.parameters(),\n",
        "                                     lr=LEARNING_RATE,\n",
        "                                     momentum=MOMENTUM,\n",
        "                                     weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        vgg16bn_scheduler = optim.lr_scheduler.CosineAnnealingLR(vgg16bn_optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "        vgg16bn_model, vgg16bn_train_losses, vgg16bn_val_losses, vgg16bn_train_accs, vgg16bn_val_accs, vgg16bn_times = train_model(\n",
        "            vgg16bn_model, criterion, vgg16bn_optimizer, vgg16bn_scheduler, num_epochs=NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        torch.save(vgg16bn_model.state_dict(), 'results/vgg16bn_cifar10.pth')\n",
        "\n",
        "        # Evaluate final test accuracy\n",
        "        vgg16bn_acc = evaluate_model(vgg16bn_model, testloader)\n",
        "        print(f\"VGG-16 BN Test Accuracy: {vgg16bn_acc:.2f}%\")\n",
        "\n",
        "        # Visualize filters and feature maps\n",
        "        visualize_filters(vgg16bn_model, \"VGG-16-BN\")\n",
        "        visualize_feature_maps(vgg16bn_model, \"VGG-16-BN\")\n",
        "\n",
        "        # Save results\n",
        "        histories.append((vgg16bn_train_accs, vgg16bn_val_accs, vgg16bn_train_losses, vgg16bn_val_losses, vgg16bn_times))\n",
        "        model_names.append(\"VGG-16 BN\")\n",
        "\n",
        "        results['VGG-16 BN'] = {\n",
        "            'parameters': count_parameters(vgg16bn_model),\n",
        "            'test_accuracy': vgg16bn_acc,\n",
        "            'train_accuracy': vgg16bn_train_accs[-1],\n",
        "            'val_accuracy': vgg16bn_val_accs[-1],\n",
        "            'time_per_epoch': sum(vgg16bn_times) / len(vgg16bn_times)\n",
        "        }\n",
        "\n",
        "    # Train and evaluate VGG-8\n",
        "    if TRAIN_VGG8:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"Training VGG-8 (Reduced Depth)\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        vgg8_model = VGG8()\n",
        "        print(f\"VGG-8 parameters: {count_parameters(vgg8_model):,}\")\n",
        "\n",
        "        vgg8_optimizer = optim.SGD(vgg8_model.parameters(),\n",
        "                                  lr=LEARNING_RATE,\n",
        "                                  momentum=MOMENTUM,\n",
        "                                  weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        vgg8_scheduler = optim.lr_scheduler.CosineAnnealingLR(vgg8_optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "        vgg8_model, vgg8_train_losses, vgg8_val_losses, vgg8_train_accs, vgg8_val_accs, vgg8_times = train_model(\n",
        "            vgg8_model, criterion, vgg8_optimizer, vgg8_scheduler, num_epochs=NUM_EPOCHS\n",
        "        )\n",
        "\n",
        "        # Save model\n",
        "        torch.save(vgg8_model.state_dict(), 'results/vgg8_cifar10.pth')\n",
        "\n",
        "        # Evaluate final test accuracy\n",
        "        vgg8_acc = evaluate_model(vgg8_model, testloader)\n",
        "        print(f\"VGG-8 Test Accuracy: {vgg8_acc:.2f}%\")\n",
        "\n",
        "        # Visualize filters and feature maps\n",
        "        visualize_filters(vgg8_model, \"VGG-8\")\n",
        "        visualize_feature_maps(vgg8_model, \"VGG-8\")\n",
        "\n",
        "        # Save results\n",
        "        histories.append((vgg8_train_accs, vgg8_val_accs, vgg8_train_losses, vgg8_val_losses, vgg8_times))\n",
        "        model_names.append(\"VGG-8\")\n",
        "\n",
        "        results['VGG-8'] = {\n",
        "            'parameters': count_parameters(vgg8_model),\n",
        "            'test_accuracy': vgg8_acc,\n",
        "            'train_accuracy': vgg8_train_accs[-1],\n",
        "            'val_accuracy': vgg8_val_accs[-1],\n",
        "            'time_per_epoch': sum(vgg8_times) / len(vgg8_times)\n",
        "        }\n",
        "\n",
        "    # Plot training histories\n",
        "    if len(histories) > 0:\n",
        "        plot_history(histories, model_names)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"SUMMARY OF RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for name, result in results.items():\n",
        "        print(f\"{name}:\")\n",
        "        print(f\"  - Parameters: {result['parameters']:,}\")\n",
        "        print(f\"  - Test Accuracy: {result['test_accuracy']:.2f}%\")\n",
        "        print(f\"  - Train Accuracy: {result['train_accuracy']:.2f}%\")\n",
        "        print(f\"  - Validation Accuracy: {result['val_accuracy']:.2f}%\")\n",
        "        print(f\"  - Avg. Time per Epoch: {result['time_per_epoch']:.2f}s\")\n",
        "        print()\n",
        "\n",
        "    # Save results to file\n",
        "    import json\n",
        "    with open('results/experiment_results.json', 'w') as f:\n",
        "        # Convert float32 to float for JSON serialization\n",
        "        serializable_results = {}\n",
        "        for model, metrics in results.items():\n",
        "            serializable_results[model] = {k: float(v) if isinstance(v, torch.Tensor) else v for k, v in metrics.items()}\n",
        "\n",
        "        json.dump(serializable_results, f, indent=4)\n",
        "\n",
        "    print(\"Results saved to 'results/experiment_results.json'\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f00c87",
      "metadata": {
        "id": "08f00c87"
      },
      "source": [
        "## RUN EXPERIMENTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c966cdd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c966cdd",
        "outputId": "e07fcd45-1123-44db-ee79-4aa238247c2d"
      },
      "outputs": [],
      "source": [
        "# Run all experiments\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_experiments()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorGPU",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
